# -*- coding: utf-8 -*-
"""Twitter_User_Tweets.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TLCzzwZqwPP6-XutmEcYshAMBU7LDWPy
"""

import tweepy
import configparser
import pandas as pd
import re
import numpy as np
from nltk.corpus import stopwords
import nltk
nltk.download('punkt')
nltk.download('wordnet')
import spacy
sp = spacy.load('en_core_web_sm')
import matplotlib.pyplot as plt; plt.rcdefaults()
import matplotlib.pyplot as plt
import seaborn as sns
from nltk.probability import FreqDist
from wordcloud import WordCloud
from PIL import Image

"""##Twitter API Set Up"""

config = configparser.ConfigParser()
config.read('config.ini')

api_key = config['twitter']['api_key']
api_secret = config['twitter']['api_secret']

access_token = config['twitter']['access_token']
access_token_secret = config['twitter']['access_token_secret']

auth = tweepy.OAuthHandler(api_key, api_secret)
auth.set_access_token(access_token, access_token_secret)

api = tweepy.API(auth)

"""# Looking at Elon Musk's Last 300 Tweets"""

user = 'ElonMusk'
limit = 300

#Tweepy to increase limit of tweets per request
tweets = tweepy.Cursor(api.user_timeline, id=user, count=500,
                       tweet_mode = 'extended').items(limit)

#Original Request but limits to 200 tweets
#tweets = api.user_timeline(screen_name=user, count=limit, 
                           #tweet_mode='extended')

#Create DataFrame
column = ['Tweet'] #columns = ['User','Tweet']
user_data = []

for tweet in tweets:
  user_data.append([tweet.full_text]) #user_data.append([tweet.user.screen_name, tweet.full_text])

user_df = pd.DataFrame(user_data, columns=column)


user_df['Tweet'].astype(str).values.tolist()

user_list = user_df['Tweet'].astype(str).tolist()

list = user_list

lines = []
for line in user_list:
  words = line.split()
  for w in words:
    lines.append(w)
print(len(lines))
print(lines)

lines

"""## Removing Punctuation

"""

lines = [re.sub(r'[^A-Za-z0-9]+', '', x) for x in lines]

lines

"""## Stemming words to their english root"""

from nltk.stem.snowball import SnowballStemmer

stemmer = SnowballStemmer(language='english')

stem = []
for word in lines:
  stem.append(stemmer.stem(word))

stem

"""## Remove Stop Words"""

stem2 = []

for word in stem:
  if word not in sp.Defaults.stop_words:
    stem2.append(word)
stem2

'''
all_stopwords = sp.Defaults.stop_words

tokenized_stem = word_tokenize(stem)
stem_without_sw = [word for word in tokenized_stem if not word in all_stopwords]

stem2 = stem_withotu_sw '''

stem2

"""## Now I will put the new list into a Series for analysis"""

df = pd.DataFrame(stem2)

df = df[0].value_counts()

freq = FreqDist()

for word in df:
  freq[word] += 1

freq

df.head()

df = df.drop('')
df = df.drop('amp')
df

df = df[:20,]
plt.figure(figsize = (10,5))
sns.barplot(df.values, df.index, alpha=0.8)
plt.title(f"Top Words Mentioned by {user}")
plt.ylabel("Word from Tweet", fontsize=12)
plt.xlabel("Frequency of Word", fontsize=12)
plt.show()

"""Here we can see that Elon has enjoyed retweeting or replying to tweets by biillym2k, TeslaOwnersSV, Spacex, Tesla, ppathol

#Setting up Donut Chart of Top 5 words
"""

df2 = df[:5,]
df2

words = df2.index.tolist()
frequency = df2.values.tolist()
print(words)
print(frequency)

colors = ['#FF0000', '#0000FF', '#FFFF00', 
          '#ADFF2F', '#FFA500']
explode = (0.05, 0.05, 0.05, 0.05, 0.05)

plt.pie(frequency, colors=colors, labels=words, autopct='%1.1f%%', pctdistance=0.85,
        explode=explode)

centre_circle = plt.Circle((0, 0), 0.70, fc='white')
fig = plt.gcf()

fig.gca().add_artist(centre_circle)

plt.title('Top 5 Words % Breakdown')

plt.show()

"""Now we can easily see the frequency of the words as a percentage"""